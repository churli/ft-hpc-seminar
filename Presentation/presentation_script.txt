1) Good morning! I am Tommaso Bianucci and today I will
	give you an overview of what is resource awareness 
	and then go a bit deeper describing a runtime system for HPC called HPX, which uses some ideas from RA.
But first I would like to quickly describe the context and the current main challanges in the HPC field.

2) Hardware.
Having a supercomputer capable of 1 exaFLOPS/s means that it is capable of performing 10^18 floating point operations per second.
A modern CPU, assuming a perfectly pipelined and compute-bound code, can reach 1-10 GFLOPS/s. So 1 exaFLOPS is equivalent to 100Millions-1Billion of these units.
This compute power will also be, as it is already currently done for petascale machines, distributed not only across nodes, but also across different types of hardware devices.
The trend for CPUs is to go towards manycore: meaning hundreds of cores on a single chip, with an on-chip network.
Then we have GPUs and FPGAs, which we have already seen in another seminar last tuesday.
Each of these devices has its own characteristics, which we must be aware of in order to properly harness their power.

3) So these machines expose an extreme degree of parallelism, which is also non-homogeneous.

4) Applications.
Not all applications like to be parallelized. There is a class of applications which is called scaling-impaired, meaning that after being parallelized on a relative low number of threads, they don't show any further speedup if new threads are added.
Applications showing an unbalanced execution tree belong to this class: this is an intrinsic load imbalance which makes it difficult to divide the application into threads.
An example are Adaptive Mesh Refinement methods, where the discretization mesh is adaptively refined in case of certain events, such as shockwaves, cracks, ecc. This cannot be known before running the application, making it difficult to properly parallelize.

5) So, as mentioned, we get poor parallel performance, but also a suboptimal resource usage, since many resources will have to wait for a slower part of the problem to be completed.

6) So some apps do not scale well, they don't like to be parallelized.
But we also have another problem: overheads of the parallelization itself.

7) This is tied to the programming model chosen for parallelizing.
In HPC we have 2 predominant models, which are often used together:
Fork-join is used for shared-memory parallelization, e.g. by OpenMP.
Shared memory means that the parallelization takes place within a single machine where all the processors/cores can access the same memory space.
FJ implies that the code has special parallel regions: when reached, the work will be spread among threads. At the end of the parallel region, a join, i.e. a synchronization operation, is performed, to resume single thread operation.

8) CSP is instead what is used as model when programming in MPI.
We have many processes, each one having its own private memory (so they can be on different machines), and they execute sequentially and perform some communication/synchronization operations with the others.
So for example a problem can be partitioned and each process works on its part and at the end they all synchronize and exchange some information.

9) These 2 models have some intrinsic problem: they use global barriers (join in FJ, barrier in CSP) and are therefore prone to load-imbalance.

10) To better understand why, let's have a look at this diagram.
We can see a parallel region is started and several threads work in parallel (the wavy arrows). Then threads start reaching the end of their work at different times because of load imbalance.
Here we can see that before performing a reduction, e.g. summing together all the results of each thread, (which implies a synchronization barrier) we need to wait for the slowest.
Then the reduction itself do not involve all the threads, therefore causing again idling in the resources.

11) So we see how limits of the semantics of a programming model can cause limits and inefficiencies in the execution.

12) How can we tackle these problems in our software?
Among the other techniques, I am now going to describe Resource Awareness.

13) Resource awareness is a broad spectrum of techniques aimed at achieving runtime adaptivity in resource allocation and usage.
So the main underlying points are:
	The application/system is able to adaptively modulate how many resources are allocated to it and how much load to put on them.
	The application/system is aware of the resources it uses and monitor those in some way.
	This monitoring and adaptation takes place at runtime instead of before, as with normal parameter tuning.

14) What do we mean with the term "resources"?
For sure we have a whole spectrum of possible hardware entities which the application can be aware of:
	Computational units: as CPU/GPU/manycore/FPGA
	Memory: properties as memory hierarchy, access speed in NUMA contexts
	Bandwidth(s): available residual bw on links, latencies, state of network devices
	I/O devices: if we can freely access disk or if it is in use
	Power or thermal: these are not directly devices but properties of the hardware which may be important to react to.

15) But we can also have software entities as resources.
As the state of buffers (e.g. in case of communication) or queues (e.g. in case of work scheduling).

16) Awareness can be at different levels and with different purposes.
We can see it at "low level", e.g. in embedded computing, when a system has to manage and react to power/aging/thermal effects.
We also have an example in automatic allocation of resources in Multi Processor Systems on Chip with Invasive computing, which is also researched in our chair: Invasive computing is about the ability for a program which is running on a core, to query the state of a neighbor core and, if free, "invade and infect" it (i.e. claim it and copy its data/code there) and use it for parallelism and release it again once done (retreat phase).

17) We can also have awareness at a "mid level", i.e. in applications or runtime systems.
This covers e.g. including features in our algorithms to deal with different states of resources.
Examples are Load Balance or task scheduling.
And of course the facilities provided by runtime systems to access resource status/statistics from an application.
An example of this is the ability to query statistics about overheads and adjust task grain size at runtime.
--> This level is where we will focus when discussing HPX.

18) At an higher level, e.g. that of a supercomputing facility, we might be interested in scheduling jobs and allocating them resources in a way to optimize throughput or energy consumption levels. That is, to keep the load within a certain band.
This is the example of what, again, the Invasive Computing research going on in our chair is working on now.
They have an MPI implementation which allows dynamic allocation/deallocaton of new processes at runtime, depending on the parallelism required by the application in that phase.
This together with a smart scheduler supporting this model, can be used to allocate jobs in a way to keep the energy consumption always within a certain band, which is something very important for supermuc.

19) HPX, stands for High Performance paralleX.

20) It uses a task-based approach, where there isn't a parallel region, but functions are invoked asynchronously (so a task is started on another thread) and the threads are synchronized only when their output is required. We will see further details on this in a minute.
It addresses both shared and distributed memory types of parallelization: so it is both for intra and inter node parallelism.
It aims to keep the overheads down as much as possible and to make it possible to use fine-grained parallelism, i.e. tasks are of small size. Which makes things easier in lots of cases.

21) Its foundational principles are:
	Async sched and lightweight synch: as we mentioned, we don't have parallel regions. We spawn tasks and we synch each one of them only when needed.
	AGAS: as potentially run on several machines, whith disjoint memory, this is an abstraction to get a virtual address space spanning all the localities. This is usually done in a partitioned fashion: i.e. the global space is partitioned statically with addresses mapping to localities. In HPX this is done in a flexible way, where the global identifier (GID) is dynamically mapped to hardware. We will see interesting consequence of this.
	Performance counters: this is a very useful infrastructure to get statistics from the runtime itself (software entities) but also from hardware: there is PAPI integration and custom counters can be easily implemented and used in any part of the application.

22) Now let's see how async exec and sync is achieved.
Through futures.
The function is invoked asynch. and immediately yields a future object. The task is then scheduled on another thread and potentially another locality.
When the result is needed, the get() method is called on the future and this triggers the sync. If the task is completed it will get the result, otherwise we will have to wait for completion.

23) How is this put together to get a different programming model?
Here we have the different localities, each with its own memory.

24) HPX adds its infrastructure: we see the AGAS covering part of the memory of each locality.
Then we have the parcelport, i.e. the message passing infrastructure.
And for each locality we have a thread scheduler.

25) Each thread scheduler manages a pool of preallocated hardware/OS threads, called the worker threads, and it feeds the various tasks to them.
This way we avoid the overhead of spawning an OS thread each time.
Since tasks return futures and can also depend on futures, we get a dependency graph which actually represent the true data dependencies.

26) Here we see the dependencies which can go across the localities. If the overhead of this system is kept in control, we can achieve better performances than the fork-join one because of reducing global synch and idle time. At the same time, we get this with a very simple code, just pass futures around and we don't need to worry about dependencies and synch explicitly.

27) So, how does HPX fit in what we talked about for RA?
Because of its features and design, HPX is quite good at supporting adaptivity, if you want to include it in your application.

28) First of all the task scheduling: it is designed to prevent starvation, i.e. when a worker thread must idle because there is no work it can do at the moment (e.g. because synch). It achieves this by work stealing: if there is an imbalance in the worker threads' queues, the thread scheduler steals work from where the load is higher and transfers it where it is slower.
This is also done in a NUMA-aware way: meaning that is done preferentially between threads belonging to the same NUMA domain.

29) The AGAS supports adaptivity because it supports dynamic relocation of objects: so we can change the physical position of an object without changing its GID. This allows moving objects transparently to the rest of the application.

30) Percolation is a way to send special tasks to specific hardware accelerators: this allows effciently using GPUs or FPGAs through OpenCL.

31) Performance counter: their infrastructure allows easy retrieval and integration of statistics from resources into the application.
Counters are objects which can be accessed globally, so it is possible to easily retrieve data even from remote resources.
An example of how PC have been used for adaptivity is automatic task-grain size.

32) HPX has good support for RA, but also some limitations which could indicate the way forward for its development.
The first one is that the HPX runtime system is, itself, not elastic, i.e. it cannot grow or shrink the worker threads or localities at runtime. It would be very interesting to get invasive capabilities built into HPX.

33) There is no support for power management and enery awareness yet. For example it would be interesting to get Dynamic Voltage and Frequency Scaling capabilities in it.

34) No effort has been made to get fault tolerance capabilities into HPX yet.

##### If there is still time!!! #####

Show the example

##### Then go to summary!

