
~\\~
\section{HPX} \label{sec:hpx}
\enlargethispage{2.5\baselineskip} % Ensure no RIGHINO on newpage!
HPX is a C++ library and runtime system for task-based parallellization. It treats both intra- and inter-node parallelization within an homogeneus interface and it adheres to the C++11/14 standard, which introduced basic support for task-based parallelism.

It features a global address space, the ability to migrate work remotely in the proximity of data, and it supports task dependencies and continuations.
Its task scheduler is designed to introduce minimal overhead and supports very fine-grained tasks down to \textasciitilde$1\mu s$\cite{grubel2016using} and work-stealing for automatic load balancing.
It also delivers a powerful performance monitoring system which allows a program to query various performance metrics at runtime and to react accordingly.
~\\~

\subsection{The ParalleX execution model}\label{subs:parallexModel}
HPX implements the ParalleX \cite{kaiser2009parallex} execution model, which leverages medium- and fine-grained task parallelism and aims at optimizing both parallel efficiency and programmability of parallel code.

The key highlights of this model are:
\begin{description}
	\item [Asynchronous task execution] Functions are meant to be called asynchronously and to yield a proxy for the actual return value. Such a proxy is called a \emph{future} (figure \ref{fig:futureFlowDiagram}). The program will need to synchronize only when the actual return value is needed, e.g. for a later computation. If the task was able to complete between the asynchronous call and the synchronization step, then no waiting time is needed.
	\item [Lightweight synchronization] Not only we can use futures but we can also make an asynchronous call depend on one or several futures: this enables the runtime system to keep track of the actual dependencies and removes the need for expensive synchronization mechanisms, such as global barriers.
	\item [Active Global Address Space (AGAS)] ParalleX features a global address space abstraction service. This address space spans all the available hardware entities, called \emph{localities}.
	The peculiarity of AGAS is that it is not a \emph{partitioned} global address space (PGAS): in PGAS the global address space is statically partitioned across the available localities and moving an object to a different locality requires a change of its address; in AGAS the address space is dynamically and adaptively mapped to the underlying localities, allowing transparent migration of objects across localities while they can still retain their \emph{global identifier (GID)}.
	\item [Message-driven queue-based scheduling] When a task execution is requested, which may be on any remote locality, an active message, called \emph{parcel}, is sent to the target locality. This triggers the creation of a \emph{PX-Thread}\footnote{From now on PX-Threads will be referred to just as \emph{threads}.}, which will be queued and then scheduled for execution on the OS thread(s) managed by the target locality. This form of message passing is therefore not limited to data and it does not require explicit receive operations to be invoked on the target side. The queuing and scheduling is designed in a way to allow for idling processors or cores to \emph{steal} work from the queues of other ones: this allows for efficient load balancing and prevents starvation
	%\todo{maybe explain starvation}
	.
\end{description}
\begin{figure}[t]
 	\begin{center}
 		\includegraphics[scale=0.3]{Figures/futureFlowDiagram.png}
 		\caption{Schematic flow diagram of a future\protect\cite{kaiser2009parallex}.
 		An asynchronous function invocation immediately returns a future, while the function is executed on another thread and potentially on another locality. On the caller thread the execution can proceed further until the .get() method is called on the future: this is when the actual synchronization with the remote operation takes place.}
 		\label{fig:futureFlowDiagram}
 	\end{center}
\end{figure}
~\\~

\subsection{HPX high-level architecture}
HPX is a runtime system implementing the ParalleX model as a C++ library and this is done by strictly adhering to C++11/14 standard interfaces for task-based parallelism.
It aims to address the four main factors that prevent scaling in scaling-impaired applications, which are referred to as \emph{SLOW}\cite{kaiser2014hpx}:
\begin{description}
	\item [Starvation] Not all resources are fully utilized because of a lack of enough concurrent work to execute.
	\item [Latencies] Intrinsic delays in accessing remote resources.
	\item [Overheads] The overheads of parallelization, i.e. the work required for management of the parallel computation and any extra work which would not be necessary in a sequential version.
	\item [Waiting for contention resolution] Any delay caused by oversubscription of shared resources.
\end{description}

HPX tries to deal with the above problems by embracing the following design principles \cite{kaiser2014hpx}:
\begin{itemize}
	\item focus on latency hiding instead of latency avoidance,
	\item embrace fine-grained parallelism instead of heavyweight threads,
	\item rediscover constraint based synchronization to replace global barriers,
	\item use adaptive locality control instead of static data distribution, 
	\item prefer moving work to data over moving data to work and 
	\item favor message driven computation over message passing.
\end{itemize}
These principles are well-known and some of them are currently used also by other parallelization-oriented languages and libraries. However HPX is the first case in which all of them are coherently embraced and exposed in its programming model \cite{kaiser2014hpx}.

Based on these principles, HPX was designed with a modular architecture composed of five high-level subsystems, each exposing a specific set of functionalities:
~\\~

\paragraph{The Threading Subsystem}
When a new thread is created, HPX queues it at an appropriate locality and it then schedules it according to configurable policies. The thread scheduling is cooperative, i.e. non-preemptive, since preemption and the overhead associated to content switches would not make sense in the context of a single application. Threads are scheduled onto a pool of OS threads, which are usually one per core, whitout requiring any kernel transition and thus removing all the overhead associated to the creation of OS threads.
~\\~

\paragraph{The Parcel Subsystem}
Parcels are the HPX implementation of active messages, i.e. messages which can not only deliver data but also trigger execution of methods on remote localities.
Parcels carry the GID of the destination, a remote action, arguments for the action (data) and, if required, a continuation.
~\\~

\paragraph{Local Control Objects}
``Local Control Objects (LCOs) control parallelization and synchronization of
HPX applications. An LCO is any object that may create, activate, or reactivate
an HPX thread.''\cite{grubel2016dynamic}

The most prominent LCOs delivered by HPX are \emph{futures} and \emph{dataflow} objects. Futures are proxies for values which might not have been computed yet and include a synchronization when the actual value is requested. Dataflow objects are instead LCOs which depend on a set of futures as input and they return themselves a future for the result of their continuation.

Futures and dataflow LCOs allow to express the true data dependencies within an application and to translate them into the associated execution tree and necessary synchronizations.
~\\~

\paragraph{Active Global Address Space}
As already mentioned in \ref{subs:parallexModel}, one of ParalleX main features is the AGAS: HPX implements an AGAS service which delivers those functionalities.
~\\~

\paragraph{The Performance Monitoring Framework}
\enlargethispage{-1\baselineskip} % Ensure no RIGHINO/VEDOVA on newpage!
HPX implements a performance monitoring framework based on a variety of \emph{performance counters}, which are objects providing metrics and statistics on the performance of
\begin{enumerate*}[label={(\roman*)}]
	\item hardware,
	\item application,
	\item HPX runtime and
	\item OS
\end{enumerate*}.
Performance counters are first class objects, they are therefore addressable by their GID from any locality and are available to both the application and the HPX runtime for performing introspection at runtime on how well the system is performing.\cite{kaiser2014hpx}
Performance counters expose a predefined interface and HPX exposes specific API functions allowing to create, manage and release counters, as well as to read their data in a structured way.
Performance counters are not only available from within the application. By passing standard command line flags to an HPX application we can conveniently have the performance counters of interest periodically logged to screen or to a file during execution. Performance counter can also be accessed in realtime by other utilities by connecting to a running HPX application through its parcel transport layer\cite{kaiser2014hpx}.
They are useful tools for performance analysis and for identifying bottlenecks, and they are even more useful as they provide the necessary infrastructure for building resource awareness into an HPX application\cite{grubel2016using}.

\begin{figure}[t]
 	\begin{center}
 		\includegraphics[scale=0.18]{Figures/hpxArchitecture.png}
 		\caption{Overview of the HPX runtime system components\protect\cite{heller2017hpx}.}
 		\label{fig:hpxArchitecture}
 	\end{center}
\end{figure}

~\\~
\section{Resource awareness in HPX} \label{sec:hpxRAC}
HPX supports resource awareness by design and, up to a certain degree, automatically.
Its execution model is based on the assumption that threading, synchronization and data distribution must not be exposed to the programmer and must be handled automatically behind the scenes. HPX provides an abstraction for parallelism which does not require the programmer to think about localities, send/receive instructions, threads or barriers, which are referred to by Hartmut Kaiser, the HPX project lead, as the ``GOTOs of parallel computing''\cite{kaiser2014goto}. HPX also encourages the programmer to define fine grained tasks, while the runtime takes care of the actual scheduling and synchronization. This has the positive side effect of making the runtime adaptive, by default, in terms of load balancing.

% HPX manages and exposes, through abstraction, both software entities and hardware entities types of resources and this abstraction can blur the boundary between these two categories.
% For example, worker threads and their respective queues obviously belong to the software entities category, however usually there is a one to one mapping between worker threads and the underlying cores or hardware threads of the machine. Although not exposed to the application, the HPX runtime system is aware of the different NUMA domains where the worker threads are allocated and this is taken into account by the task scheduler.

% In general we can define a resource in HPX as any entity which can be monitored through a performance counter. This includes services offered by the runtime system itself, as the parcel or the threading subsystems, but also OS resources consumed by HPX in all localities, such as memory, IO or network.
HPX manages hardware resources and exposes them through an abstraction process according to its execution model. With the exception of percolation, which is described later in this section, in HPX we can consider as resources all these abstractions provided by the runtime system.
For example worker threads, localities and the parcel subsistem are respectively abstractions of processing units, compute nodes and the network infrastructure. Task queues are instead a software entity type of resource which does not correspond to any hardware resource but represents the load associated with a processing unit.

It is also important to consider that the performance monitoring framework --- i.e. the infrastructure allowing an application to retrieve performance statistics about the underlying resources and which enables adaptivity on the application side --- can be extended by new, user-defined, performance counters, thus potentially broadening the set of resources the application can be aware of.

No comprehensive analysis of resource awareness in HPX has been published so far.
Therefore I below summarize the capabilities of HPX --- as reported in scientific literature --- in support for resource awareness together with the factors which currently limit the scope of its adaptivity.
~\\~

\subsection{Capabilities}
\paragraph{Task scheduling}
The HPX thread manager and the way it schedules queues for each worker thread are configurable with three main pre-defined policies\cite{heller2017hpx,amatya2014parallel}:
\begin{description}
	\item [Static] The thread manager maintains a work queue for each worker thread and tasks are distributed to queues in a round robin fashion. In this policy there is no way for tasks to change their allocation to queues.
	\item [Local] This is the default scheduling policy. In this policy the thread manager maintains a work queue for each worker thread plus extra special queues for high and low priority tasks. Work is distributed to queues as in the static policy, but as soon as a load imbalance is detected on the worker threads, work is stolen from higher loaded threads and distributed to lower loaded ones.
	\item [Hierarchical] In this policy the thread manager maintains a tree of work queues where the leaves correspond to the worker threads. New tasks are always enqueued at the root of the tree and then trickle down the hierarchy. In this policy work can be stolen at any level of the tree, from queues belonging to the same level.
\end{description}

From the above policies we can see one very important behaviour of the task scheduler which allows for resource awareness when enabled: \emph{work stealing}.

As mentioned, work stealing occurs as soon as a load imbalance is detected among different worker threads' queues.
This is optionally done in a NUMA-aware way, i.e. the work is redistributed preferentially within the same NUMA domain in order to preserve memory access performance.

Work stealing therefore operates an automatic load balancing action at runtime and is therefore a means to get adaptivity out of the box with HPX.

An important factor that has to be taken into account for effective task scheduling is data --- both temporal and spatial --- locality. Performing task scheduling blind to data locality can lead to substantial performance degradation \cite{connelly1994workload}, while locality aware scheduling can effectively reduce the number of data load and transfer operations and their associated overhead.

HPX can include data locality awareness in task scheduling and its task dependency semantics allows hierarchical grouping of tasks, therefore naturally exposing different levels of data locality \cite{amatya2014parallel}.
~\\~

\paragraph{AGAS}
Although AGAS does not a priori imply resource awareness, it is an important feature within HPX in support for adaptivity. It ``is a dynamic and adaptive address space
which evolves over the lifetime of an application'' and ``is the foundation for dynamic locality control as objects can be moved to other localities without changing their global address''.\cite{kaiser2014hpx}
This allows implementing mechanisms to relocate objects to different localities in response to performance metrics and resource constraints \cite{amatya2014parallel} and still being able to access them from the rest of the application in a transparent way. The migration decision has to be based on performance metrics and be aware of resources also for what concerns the target locality, in order to ensure a better performance as outcome. This requires a performance monitoring infrastructure allowing access to metrics for remote localities, which in HPX is fullfilled by performance counters.
The object relocation logic can be therefore completely decoupled from the code accessing the objects and this clearly allows a much cleaner and modular code and it fullfills the underlying design goal of having parallelization and data distribution internalities as much hidden from the programmer as possible.
~\\~

\paragraph{Percolation}
Percolation is a special use of parcels which allows directly targeting hardware resources, instead of logical data objects, as destination.
Percolation provides therefore a way to address specialized hardware --- such as gpGPUs and FPGAs --- directly through the parcel layer, enabling to provide work to these accelerators as well as getting back the results in a clean way.
The percolation mechanism in HPX is aware of the characteristics of specialized hardware resources and can therefore route relevant tasks to these resources in a efficient way \cite{amatya2014parallel}.
~\\~

\paragraph{Performance counters}
Performance counters are a central feature of HPX and play a key role in its resource awareness capabilities.

An efficient performance measurement framework is a requirement for any runtime system targeting fine grained parallelism and exascale computing: it needs to be able to gather and manage a massive amount of information, while at the same time being as unobtrusive as possible, in order to minimize its perturbations to the application performance. Furthermore, such a measurement framework needs to be able to scale as required by applications and available resources \cite{amatya2014parallel}.

Traditionally performance measurements are used for post-run analysis, for either debugging or optimization of parameters.
However as the degree of parallelism increases --- and so do performance-affecting parameters --- it becomes more and more crucial to have well-integrated dynamic performance measurement capabilities and to leverage the measured data for dynamic tuning of application parameters.

HPX performance monitoring framework was designed having runtime adaptivity in mind \cite{kaiser2014hpx} and they can indeed support it in a convenient way.

An example of how an application could consume performance counters to enhance its resource awareness is given in \cite{grubel2016dynamic}:
``[...] an application that transfers data over the network could consume counter data from a network switch to determine how much data to transfer without competing for network bandwidth with other network traffic. The application could use the counter data to adjust its transfer rate as the bandwidth usage from other network traffic increases or decreases.''

Furthermore, as shown in a proof of concept by Grubel \cite{grubel2016dynamic}, the HPX performance monitoring framework can be used, together with an additional library and additional changes in the application code, to achieve automatic tuning of task grain size. This kind of feedback loop allows the application to react to real-time performance metrics such as task scheduling overheads to dynamically adapt its behaviour to achieve better performance.

Another interesting work which involves adaptivity mediated by performance counters is about \emph{smart executors} in HPX. Khatami et al \cite{khatami2017hpx} showed how loop parallelization efficiency can be tuned, at runtime, using machine learning on a combination of static information gathered at compile time and dynamic information captured at runtime. Their tests on HPX parallel algorithms\footnote{HPX implements parallel algorithms as defined in C++ Standard.} showed a speedup improvement of $12\%$~\textasciitilde~$35\%$ for the Matrix Multiplication, Stream and 2D Stencil benchmarks with respect to the standard HPX implementation.
~\\~

\subsection{Limitations} \label{subsec:limitations}
\paragraph{(An)elasticity of HPX}
Although we have seen that an application can be elastic within HPX, HPX itself does not currently support any variation of the number of worker threads or localities at runtime. As already mentioned in section \ref{sec:resourceAwareness}, this capability, coupled to a process manager capable of leveraging it, can improve resource usage and throughput of a supercomputer.

Projects as Elastic MPI --- an MPI extension implementing invasive computing semantics\cite{urena2017resource} --- and Adaptive MPI --- which implements MPI ranks as Charm++ user level threads\cite{gupta2014towards,prabhakaran2015batch} --- are examples of already ongoing research in this direction.
~\\~

\paragraph{Power management}
Although HPX can lead to a more efficient power consumption due to its load balancing capabilities, it does not explicitly support any power management facility.

The current trend in supercomputing is to pay more and more attention to energy efficiency, as current top tier supercomputers already have energy requirements in the order of 10MW. Research efforts are currently going into the development of more energy efficient hardware, as well as integrating energy management capabilities into the software\cite{surve2013energy}.

Energy awareness requires the ability to read metrics as temperature, clock frequency and power consumption from the underlying hardware and to react with strategies as Dynamic Voltage and Frequency Scaling (DVFS).
~\\~

\paragraph{Fault tolerance}
Increasing the parallelism of a system implies an increase in the number of hardware devices involved in a computation. This increases the chances of incurring in a component failure. Current and future HPC systems must therefore include means to recover the intermediate state of a computation prior to the failure and to resume the task from that point.

HPX does not currently support any facility for checkpointing or for recovery from partial data losses.

% [\TODO Add something on LIMITATIONS here!]
%eof
